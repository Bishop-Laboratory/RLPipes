
if paired_end:
    # Name sorted bams are required for bam to fastq step in paired-end reads
    # rule collate_bam:
    #     input: "{outdir}/bams/{sample}.{genome}.bam"
    #     output: "{outdir}/bams_name_sort/{sample}.{genome}.bam"
    #     threads: SAMPLES['cores'][0]
    #     shell: "samtools collate -@ {threads} -o {output} {input}"
    # Partially adapted from https://www.biostars.org/p/15235/
    # rule bam_head_to_fastq_pe:
    #     input: "{outdir}/bams_name_sort_truncated/{sample}.{genome}.bam"
    #     output:
    #         R1="{outdir}/fastq_head/{sample}.{genome}_R1.fastq",
    #         R2="{outdir}/fastq_head/{sample}.{genome}_R2.fastq"
    #     shell: "bedtools bamtofastq -i {input} -fq {output.R1} -fq2 {output.R2}"
    # Infer library type

else:
    rule bam_head_to_fastq_se:
        input: "{outdir}/bams_name_sort/{sample}.{genome}.bam"
        output: "{outdir}/fastq_head/{sample}.{genome}.fastq"
        shell: "bedtools bamtofastq -i {input} -fq {output}"
# Extra rules I couldn't bear to part with


# Proposal -- there are markers which should prove whether reads are stranded or not.
# These can be determined from stranded data and used to predict strandedness with salmon? or with a machine model
rule check_strandedness:
    input: "{outdir}/bams/{sample}.{genome}.bam",
    output: "{outdir}/info/{sample}.{genome}.stranded.txt"
    shell: ""
rule pe_bam_to_small_fastq:
        input: "{outdir}/bams/{sample}.{genome}.bam"
        output:
            R1="{outdir}/fastq_head/{sample}.{genome}_R1.fastq",
            R2="{outdir}/fastq_head/{sample}.{genome}_R2.fastq"
        threads: SAMPLES['cores'][0]
        shell: "samtools collate -f -O --output-fmt SAM -@ {threads} {input}"\
                + " | head -n 500000 | samtools fastq -@ {threads} -1 {output.R1} -2 {output.R2} - || true"
rule salmon_determine_library_type_pe:
    input:
        R1="{outdir}/fastq_head/{sample}.{genome}_R1.fastq",
        R2="{outdir}/fastq_head/{sample}.{genome}_R2.fastq",
        index=SAMPLES['genome_dir'][0] + "/{genome}/salmon_index/"
    output:
        "{outdir}/libtype_infer/{sample}.{genome}/lib_format_counts.json"
    threads: SAMPLES['cores'][0]
    shell:
        "salmon quant -l A -i {input.index} --skipQuant -1 {input.R1} -2 {input.R2} -p {threads} -o"\
        + " {wildcards.outdir}/libtype_infer/{wildcards.sample}.{wildcards.genome}/"


rule qualimap:
    input:
        "{outdir}/bams/{sample}.{genome}.bam"
    output:
        "{outdir}/QC/bam/{sample}/qualimap.html"
    params:
        extra="-nr 10000 --java-mem-size=80G -nw 200"
    threads: SAMPLES['cores'][0]
    shell: "qualimap bamqc -bam {input} {params.extra} -nt {threads} -outdir {outdir}/QC/bam/{sample}"


rule prepare_transcriptome:
    output:
        SAMPLES['genome_dir'][0] + "/{genome}/{genome}.transcriptome.fa"
    input:
        fasta=SAMPLES['genome_dir'][0] + "/{genome}/{genome}.fa",
        gtf=SAMPLES['genome_dir'][0] + "/{genome}/{genome}.gtf"
    shell: "gffread {input.gtf} -g {input.fasta} -w {output}"


rule prepare_transcriptome:
    output:
        SAMPLES['genome_dir'][0] + "/{genome}/{genome}.transcripts.fa"
    input:
        fasta=SAMPLES['genome_dir'][0] + "/{genome}/{genome}.fa",
        gtf=SAMPLES['genome_dir'][0] + "/{genome}/{genome}.gtf"
    threads: SAMPLES['cores'][0]
    shell: "rsem-prepare-reference  --gtf {input.gtf} -p {threads} {input.fasta} "\
         + SAMPLES['genome_dir'][0] + "/{wildcards.genome}/{wildcards.genome}"

rule prepare_salmon_index:
    input: SAMPLES['genome_dir'][0] + "/{genome}/{genome}.transcripts.fa"
    output: SAMPLES['genome_dir'][0] + "/{genome}/salmon_index/info.json"
    threads: SAMPLES['cores'][0]
    shell: "salmon index -t {input} -p {threads} -i " + SAMPLES['genome_dir'][0]\
         + "/{wildcards.genome}/salmon_index/"

rule get_read_lengths:
    input: "{outdir}/bams/{sample}.{genome}.bam",
    output: "{outdir}/info/{sample}.{genome}.read.lengths.txt",
    run:
        import pysam
        # bamfile = pysam.AlignmentFile(input)
        bamfile = pysam.AlignmentFile("RSeq_out/bams/SRX6427720_DRB_qDRIP-seq_1.hg38.bam")
        count = 0
        read_lens = []
        for read in bamfile.fetch():
            read = read.get_forward_sequence()
            read_lens.append(len(read))
            count += 1
            if count > 10000:
                break
        bamfile.close()
        mean_read_len = sum(read_lens)/len(read_lens)

rule mark_duplicates:
    input: "{outdir}/bam_dups/{sample}.{genome}.{exp_type}.bam"
    output:
        bam="{outdir}/bams/{sample}.{genome}.{exp_type}.bam",
        metrics="{outdir}/info/{sample}.{genome}.{exp_type}.duplicate.metrics.txt"
    params:
        ""
    threads: cores
    shell: """
        picard MarkDuplicates I={input} O={output.bam} M={output.metrics}
    """



